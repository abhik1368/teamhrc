{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"../../hillary_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CountsByKeyword(df, col, person, topics):\n",
    "    \"\"\"\n",
    "    Returns a dict of total mention counts per keyword for the given person.\n",
    "    'By' parameter controls which field you're getting counts by.\n",
    "    Big return says: return a dictionary via comprehension for lists, or just a dict for one value\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(topics, (str, unicode, list)): \n",
    "        raise TypeError('\\'topics\\' parameter must be either str or list') \n",
    "    \n",
    "    person = '(' + person + ')'\n",
    "    \n",
    "    return (\n",
    "        {topic: df[col].loc[\n",
    "                (df[col].str.contains(person, case = False)) \n",
    "                & (df['ExtractedBodyText'].str.contains(topic, case = False))].count()\n",
    "            for topic in topics} \n",
    "        if isinstance(topics, list) \n",
    "        else {topics: df[col].loc[\n",
    "                (df[col].str.contains(person, case = False))\n",
    "                & (df['ExtractedBodyText'].str.contains(topics, case = False))].count()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacoblehrhoff/anaconda/lib/python2.7/site-packages/pandas/core/strings.py:207: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \" groups, use str.extract.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'blumenthal': 20}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function above\n",
    "CountsByKeyword(\n",
    "    emails, \n",
    "    col = 'MetadataFrom', \n",
    "    person = '.*', \n",
    "    topics = 'blumenthal'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     NaN\n",
       "1       B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...\n",
       "2                                                     Thx\n",
       "3                                                     NaN\n",
       "4       H <hrod17@clintonemail.com>\\nFriday, March 11,...\n",
       "5       Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...\n",
       "6                                                     NaN\n",
       "7       H <hrod17@clintonemail.corn>\\nFriday, March 11...\n",
       "8                                                     FYI\n",
       "9       B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd...\n",
       "10                                           Fyi\\nB6\\n— —\n",
       "11      B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd...\n",
       "12                                                    Fyi\n",
       "13      Anne-Marie Slaughter\\nSunday, March 13, 2011 9...\n",
       "14      _ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...\n",
       "15      I asked to attend your svtc today with Embassy...\n",
       "16                   Hope. See picture below Kamala sent.\n",
       "17                                         Another photo.\n",
       "18                                          This is nice.\n",
       "19      Amazing.\\nSullivan, Jacob J <Sullivanii@state,...\n",
       "20      H <hrod17@clintonernaii.com›\\nWednesday, Septe...\n",
       "21      Pis print.\\nH < hrod17@clintoriernail.corn>\\nW...\n",
       "22                                             Pis print.\n",
       "23      Follow Up Flag: Follow up\\nFlag Status: Flagge...\n",
       "24                                                     B5\n",
       "25      Sidney Blumenthal\\nThursday, September 13, 201...\n",
       "26                                               Will do.\n",
       "27                                   Remind me to discuss\n",
       "28      http://religion.b1ogs.cnn.com/20 1 2/09/13/my-...\n",
       "29                                         See note below\n",
       "                              ...                        \n",
       "7915    sbwhoeop\\nTuesday, December 14, 2010 9:35 AM\\n...\n",
       "7916    Death of Holbrookell.y,J3_4.:5 _043.) ,:u1.01 ...\n",
       "7917                                                  NaN\n",
       "7918                                                  NaN\n",
       "7919    on: 12/13/2025\\nFrom\\nTo: Verveer, Melanne S\\n...\n",
       "7920    That was what lona had when she went in to tal...\n",
       "7921                                                  Fyi\n",
       "7922    Madame Secretary, I wanted to flag for you tha...\n",
       "7923    It's at the bottom of page 1:\\n...Ambassador R...\n",
       "7924    I will be at a USGLC breakfast doing qddr so w...\n",
       "7925    Passed the Senate 81 to 19. START vote on moti...\n",
       "7926                                                   -^\n",
       "7927                         Traffic below from bottom up\n",
       "7928                               fyi\\nForwarded message\n",
       "7929    Agree — he did all the writing and integrating...\n",
       "7930    Nice\\nForgot to tell you about our harrowing c...\n",
       "7931                                                  NaN\n",
       "7932                                        Worth a read.\n",
       "7933    The unsigned editorials will appear in all of ...\n",
       "7934                                                  NaN\n",
       "7935                                                  NaN\n",
       "7936    Mills, Cheryl D <MillsCD@state.gov>\\nThursday,...\n",
       "7937    THE NBC/YORKER\\nDecember 14, 2010\\nThe Envoy\\n...\n",
       "7938    Hi. Sorry I haven't had a chance to see you, b...\n",
       "7939    B6\\nI assume you saw this by now -- if not, it...\n",
       "7940                                                  NaN\n",
       "7941    Big change of plans in the Senate. Senator Rei...\n",
       "7942                                                  NaN\n",
       "7943    PVerveer B6\\nFriday, December 17, 2010 12:12 A...\n",
       "7944                                           See below.\n",
       "Name: ExtractedBodyText, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.ExtractedBodyText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To lowercase - not necessary\n",
    "emails.CleanedBody = emails.ExtractedBodyText.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'h',\n",
       " 'latest',\n",
       " 'how',\n",
       " 'syria',\n",
       " 'is',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'for',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional parser - removes punctuation and numbers!\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "test = tokenizer.tokenize(emails.CleanedBody[1])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'h',\n",
       " 'latest',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional! Remove stop words\n",
    "\n",
    "filtered_words = [word for word in test if word not in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-686b5fe980ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtag_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtag_dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mw_td_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_td_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_short_binstring\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_short_binstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSHORT_BINSTRING\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_short_binstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag_dic = [nltk.pos_tag(word) for word in filtered_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'b']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broken Stemmer. When working turns hillary into \"hillari\". Might be a problem...\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stemmed_test = []\n",
    "for word in test[0]:\n",
    "    stems = stemmer.stem(word)\n",
    "    stemmed_test.append(stems)\n",
    "stemmed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 1}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broken count dictionary\n",
    "\n",
    "count = {} # initialize dictionary\n",
    "for word in test[0]:\n",
    "    word = word.lower() # normalize case\n",
    "    if word not in count: # previously unseen word?\n",
    "        count[word] = 0 # if so set count to 0\n",
    "        count[word] += 1 # increment word count\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk.classify.util\n",
    "# from nltk.classify import NaiveBayesClassifier\n",
    "# from nltk.corpus import movie_reviews\n",
    " \n",
    "# def word_feats(words):\n",
    "#     return dict([(word, True) for word in words])\n",
    " \n",
    "# negids = movie_reviews.fileids('neg')\n",
    "# posids = movie_reviews.fileids('pos')\n",
    " \n",
    "# negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "# posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "# negcutoff = len(negfeats)*3/4\n",
    "# poscutoff = len(posfeats)*3/4\n",
    " \n",
    "# trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "# testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "# print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "# classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "# print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "# classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', u'NN'),\n",
       " ('..', u'NN'),\n",
       " ('From', u'IN'),\n",
       " ('Randolph', u'NNP'),\n",
       " ('Lawrence', u'NNP'),\n",
       " ('M', u'NNP'),\n",
       " ('Sent', u'NNP'),\n",
       " ('Wednesday', u'NNP'),\n",
       " ('September', u'NNP'),\n",
       " ('12', u'CD'),\n",
       " ('2012', u'CD'),\n",
       " ('04:33', u'CD'),\n",
       " ('PM', u'NNPS'),\n",
       " ('To', u'TO'),\n",
       " ('Mills', u'NNS'),\n",
       " ('Cheryl', u'NNP'),\n",
       " ('D', u'NNP'),\n",
       " ('Subject', u'NNP'),\n",
       " ('RE', u'NN'),\n",
       " ('Not', u'RB'),\n",
       " ('a', u'DT'),\n",
       " ('dry', u'JJ'),\n",
       " ('eye', u'NN'),\n",
       " ('in', u'IN'),\n",
       " ('NEA', u'NNP'),\n",
       " ('Including', u'NNP'),\n",
       " ('mine', u'NN'),\n",
       " ('Her', u'PRP$'),\n",
       " ('remarks', u'NNS'),\n",
       " ('were', u'VBD'),\n",
       " ('really', u'RB'),\n",
       " ('moving', u'VBG'),\n",
       " ('Chriswas', u'NNP'),\n",
       " ('an', u'DT'),\n",
       " ('amazing', u'JJ'),\n",
       " ('man', u'NN'),\n",
       " ('Such', u'PDT'),\n",
       " ('a', u'DT'),\n",
       " ('huge', u'JJ'),\n",
       " ('loss', u'NN'),\n",
       " ('You', u'PRP'),\n",
       " ('know', u'VBP'),\n",
       " ('I', u'PRP'),\n",
       " ('was', u'VBD'),\n",
       " ('in', u'IN'),\n",
       " ('Libya', u'NNP'),\n",
       " ('before', u'IN'),\n",
       " ('coming', u'VBG'),\n",
       " ('here', u'RB'),\n",
       " ('and', u'CC'),\n",
       " ('in', u'IN'),\n",
       " ('my', u'PRP$'),\n",
       " ('almost', u'RB'),\n",
       " ('ten', u'JJ'),\n",
       " ('years', u'NNS'),\n",
       " ('I', u'PRP'),\n",
       " ('have', u'VBP'),\n",
       " ('never', u'RB'),\n",
       " ('worked', u'VBN'),\n",
       " ('with', u'IN'),\n",
       " ('such', u'PDT'),\n",
       " ('a', u'DT'),\n",
       " ('calm', u'NN'),\n",
       " ('cool', u'NN'),\n",
       " ('headed', u'VBD'),\n",
       " ('funny', u'JJ'),\n",
       " ('diplomat', u'NN'),\n",
       " ('Made', u'VBN'),\n",
       " ('it', u'PRP'),\n",
       " ('all', u'DT'),\n",
       " ('seem', u'VBP'),\n",
       " ('really', u'RB'),\n",
       " ('easy-', u'JJ'),\n",
       " ('even', u'RB'),\n",
       " ('in', u'IN'),\n",
       " ('one', u'CD'),\n",
       " ('of', u'IN'),\n",
       " ('the', u'DT'),\n",
       " ('hardest', u'JJS'),\n",
       " ('places', u'NNS'),\n",
       " ('to', u'TO'),\n",
       " ('work', u'VB'),\n",
       " ('in', u'IN'),\n",
       " ('the', u'DT'),\n",
       " ('world', u'NN')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextBlob tagger\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(emails.ExtractedBodyText[14])\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'_ ... ..', 'randolph', 'lawrence', 'september', 'pm', 'mills', u'cheryl d subject', 're', u'dry eye', u'nea including', 'chriswas', u'amazing man', u'huge loss', 'libya', u'funny diplomat', 'made', u'hardest places'])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print nouns\n",
    "\n",
    "blob.noun_phrases   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06666666666666667, 0.2, 0.6000000000000001, 0.20000000000000004, 0.18, 0.43333333333333335]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25777777777777783"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average sentiment across sentences\n",
    "\n",
    "import numpy as np\n",
    "sentiment = []\n",
    "for sentence in blob.sentences:\n",
    "    sentiment.append(sentence.sentiment.polarity)\n",
    "print sentiment\n",
    "np.mean(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(len(emails.ExtractedBodyText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <type 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-46133618d63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtractedBodyText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtractedBodyText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0memails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtractedBodyText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jacoblehrhoff/anaconda/lib/python2.7/site-packages/textblob/blob.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             raise TypeError('The `text` argument passed to `__init__(text)` '\n\u001b[0;32m--> 346\u001b[0;31m                             'must be a string, not {0}'.format(type(text)))\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             raise NotImplementedError(\"clean_html has been deprecated. \"\n",
      "\u001b[0;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <type 'float'>"
     ]
    }
   ],
   "source": [
    "for x in range(len(emails.ExtractedBodyText)):\n",
    "    if type(emails.ExtractedBodyText[x] == str):\n",
    "        emails['tags'][x] = TextBlob(emails.ExtractedBodyText[x]).tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "emails.testBody = emails.ExtractedBodyText.apply(lambda x: str(x)).apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-74-6fb20c637d5a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-74-6fb20c637d5a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    emails.CleanedBody = emails.CleanedBody.apply(lambda x: nltk.sent_tokenize(x)\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "emails.CleanedBody = emails.CleanedBody.apply(lambda x: nltk.sent_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
